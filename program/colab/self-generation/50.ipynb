{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyMdLFs3zxiRal8h2EFPf0+q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qQpOv6JFvEvQ","executionInfo":{"status":"ok","timestamp":1706221552501,"user_tz":-420,"elapsed":18399296,"user":{"displayName":"T. Februanto","userId":"03943959343087477946"}},"outputId":"54629457-6f84-4873-b1bd-948c1db75d2c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/aaa/datasets/db_bench/fillrandom_1/iolt_1.csv\n","75.44 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/fillrandom_1/iolt_2.csv\n","44.15 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/fillrandom_1/qlt_1.csv\n","80.0 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/fillrandom_1/qlt_2.csv\n","80.0 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/fillrandom_2/iolt_1.csv\n","81.48 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/fillrandom_2/iolt_2.csv\n","91.67 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/fillrandom_2/qlt.csv\n","80.0 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/fillseq/iolt_1.csv\n","82.18 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/fillseq/iolt_2.csv\n","90.03 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/fillseq/qlt.csv\n","100.0 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/mixgraph/iolt.csv\n","91.51 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/mixgraph/qlt.csv\n","73.88 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/read_1/bclt.csv\n","72.6 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/read_1/qlt.csv\n","80.0 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/read_2/bclt.csv\n","72.56 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/read_2/qlt.csv\n","80.0 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readrandom_1/iolt.csv\n","61.54 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readrandom_1/qlt.csv\n","80.0 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readrandom_2/qlt.csv\n","64.01 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readrandom_3/qlt.csv\n","64.01 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readrandom_4/bclt.csv\n","67.39 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readrandom_4/iolt.csv\n","63.14 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readrandom_4/qlt.csv\n","64.02 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readrandom_5/bclt.csv\n","73.36 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readrandom_5/qlt.csv\n","80.0 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readrandom_6/qlt.csv\n","100.0 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readrandom_7/qlt.csv\n","80.0 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readrandom_8/bclt.csv\n","72.62 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readrandom_8/qlt.csv\n","80.0 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readrandom_9/bclt_1.csv\n","85.89 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readrandom_9/bclt_2.csv\n","82.1 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readrandom_9/bclt_3.csv\n","82.06 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readrandom_9/bclt_4.csv\n","82.07 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readrandom_9/bclt_5.csv\n","82.13 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readrandom_9/bclt_6.csv\n","82.06 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readrandom_9/bclt_7.csv\n","82.09 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readrandom_9/bclt_8.csv\n","82.04 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readrandom_9/iolt.csv\n","65.47 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readrandom_9/qlt.csv\n","80.0 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readwrite_1/bclt.csv\n","90.15 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readwrite_1/bclt_replay.csv\n","82.75 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readwrite_1/iolt.csv\n","72.26 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readwrite_1/iolt_replay.csv\n","54.7 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readwrite_1/qlt.csv\n","80.0 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readwrite_2/bclt_1.csv\n","67.44 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readwrite_2/bclt_2.csv\n","64.64 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readwrite_2/iolt.csv\n","39.27 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readwrite_2/qlt.csv\n","80.0 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/readwrite_3/qlt.csv\n","80.0 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/updaterandom_1/bclt.csv\n","77.47 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/updaterandom_1/qlt.csv\n","60.79 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/updaterandom_2/bclt.csv\n","77.49 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/updaterandom_2/qlt.csv\n","60.85 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/updaterandom_3/bclt.csv\n","77.43 %\n","\n","/content/drive/MyDrive/aaa/datasets/db_bench/updaterandom_3/qlt.csv\n","60.65 %\n","\n","/content/drive/MyDrive/aaa/datasets/redis/bclt.csv\n","99.67 %\n","\n","/content/drive/MyDrive/aaa/datasets/redis/iolt.csv\n","63.73 %\n","\n","/content/drive/MyDrive/aaa/datasets/redis/qlt.csv\n","100.0 %\n","\n","/content/drive/MyDrive/aaa/datasets/ycsb/workloada/bclt_1.csv\n","57.34 %\n","\n","/content/drive/MyDrive/aaa/datasets/ycsb/workloada/bclt_2.csv\n","53.75 %\n","\n","/content/drive/MyDrive/aaa/datasets/ycsb/workloada/bclt_3.csv\n","52.3 %\n","\n","/content/drive/MyDrive/aaa/datasets/ycsb/workloada/bclt_4.csv\n","51.1 %\n","\n","/content/drive/MyDrive/aaa/datasets/ycsb/workloada/bclt_5.csv\n","54.02 %\n","\n","/content/drive/MyDrive/aaa/datasets/ycsb/workloada/bclt_6.csv\n","54.55 %\n","\n","/content/drive/MyDrive/aaa/datasets/ycsb/workloada/bclt_7.csv\n","53.13 %\n","\n","/content/drive/MyDrive/aaa/datasets/ycsb/workloada/qlt_1.csv\n","85.37 %\n","\n","/content/drive/MyDrive/aaa/datasets/ycsb/workloada/qlt_2.csv\n","87.7 %\n","\n","/content/drive/MyDrive/aaa/datasets/ycsb/workloadc/bclt.csv\n","76.7 %\n","\n","/content/drive/MyDrive/aaa/datasets/ycsb/workloadc/qlt.csv\n","84.5 %\n","\n"]}],"source":["from google.colab import drive\n","from natsort import natsorted\n","from sklearn.linear_model import LinearRegression, LogisticRegression\n","from sklearn.metrics import accuracy_score, r2_score\n","import gc\n","import numpy as np\n","import os\n","import pandas as pd\n","import warnings\n","drive.mount('/content/drive')\n","warnings.filterwarnings('ignore')\n","root_folder = '/content/drive/MyDrive/aaa/datasets'\n","def find_and_sort_csv_files(folder):\n","    csv_files = []\n","    for root, dirs, files in os.walk(folder):\n","        for file in files:\n","            if file.endswith('.csv'):\n","                csv_files.append(os.path.join(root, file))\n","    csv_files_sorted = natsorted(csv_files)\n","    return csv_files_sorted\n","all_csv_files_sorted = find_and_sort_csv_files(root_folder)\n","num_past_measurements = 20\n","feature_name = 'HISTORY' ########################################################\n","test_ratio_in_percentage = 50 ########################################################\n","test_ratio = test_ratio_in_percentage / 100 ########################################################\n","def process_dataset(file_path):\n","    data = pd.read_csv(file_path)\n","    data = data.head(int(np.round(len(data) * 0.1)))\n","    data = data.reset_index()\n","    test_size = int(np.round(len(data) * test_ratio))\n","    train_data, test_data = data[:-test_size], data[-test_size:]\n","    num_columns = train_data.shape[1]\n","    prediction_indices = range(len(train_data), len(train_data) + test_size)\n","    prediction_data = pd.DataFrame(prediction_indices, columns=['index'], index=prediction_indices)\n","    for col in [column for column in train_data.columns if train_data[column].nunique() == 1]:\n","        prediction_data[col] = pd.Series(train_data[col].iloc[0]).repeat(test_size).values\n","    columns_not_in_prediction = [col for col in train_data.columns.tolist() if col not in prediction_data.columns.tolist()]\n","    unique_values_dict = {}\n","    for col in columns_not_in_prediction:\n","        unique_values = train_data[col].nunique()\n","        unique_values_dict[col] = unique_values\n","    continuous_columns = []\n","    discrete_columns = []\n","    for col, unique_values in unique_values_dict.items():\n","        if unique_values > 500:\n","            continuous_columns.append(col)\n","        else:\n","            discrete_columns.append(col)\n","    start_index = len(data) - test_size\n","    for col in columns_not_in_prediction:\n","        for i in range(1, num_past_measurements + 1):\n","            data[f'{col}_{i}'] = data[col].shift(i * 25)\n","            if data[f'{col}_{i}'].dtype == 'int64':\n","                data[f'{col}_{i}'].fillna(-1, inplace=True)\n","            elif data[f'{col}_{i}'].dtype == 'float64':\n","                data[f'{col}_{i}'].fillna(-1.0, inplace=True)\n","            if data[f'{col}_{i}'].dtype != data[col].dtype:\n","                data[f'{col}_{i}'] = data[f'{col}_{i}'].astype(data[col].dtype)\n","            train_data.loc[:start_index, f'{col}_{i}'] = data.loc[:start_index, f'{col}_{i}']\n","            test_data.loc[:start_index, f'{col}_{i}'] = data.loc[:start_index, f'{col}_{i}']\n","            prediction_data.loc[start_index:, f'{col}_{i}'] = data.loc[start_index:, f'{col}_{i}']\n","    num_rows = int(np.round(len(train_data) * 0.05))\n","    for col in columns_not_in_prediction:\n","        features = [f'{col}_{i}' for i in range(1, num_columns)]\n","        X_test = prediction_data[features]\n","        if col in continuous_columns:\n","            X_train = train_data[features].values[-num_rows:]\n","            y_train = train_data[col].values[-num_rows:]\n","            model = LinearRegression()\n","        else:\n","            X_train = train_data[features]\n","            y_train = train_data[col]\n","            model = LogisticRegression(max_iter=1000)\n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_test)\n","        y_pred = np.where(y_pred < -1, -1, y_pred)\n","        if train_data[col].dtype == np.int64:\n","            y_pred = np.round(y_pred).astype(np.int64)\n","        prediction_data[col] = y_pred\n","    columns_to_drop = ['index']\n","    if columns_not_in_prediction:\n","        start_index = train_data.columns.get_loc(f'{columns_not_in_prediction[0]}_1')\n","        end_index = train_data.columns.get_loc(f'{columns_not_in_prediction[-1]}_{num_past_measurements}')\n","        columns_to_drop.extend(data.columns[start_index:end_index + 1])\n","    data = data.drop(columns_to_drop, axis=1)\n","    train_data = train_data.drop(columns_to_drop, axis=1)\n","    test_data = test_data.drop(columns_to_drop, axis=1)\n","    prediction_data = prediction_data.drop(columns_to_drop, axis=1)\n","    prediction_data = prediction_data[train_data.columns]\n","    scores = {}\n","    for col in prediction_data.columns:\n","        if col in continuous_columns:\n","            score = r2_score(test_data[col], prediction_data[col])\n","            if score < 0:\n","                score = 0\n","            metric = 'R-squared'\n","        else:\n","            score = accuracy_score(test_data[col], prediction_data[col])\n","            metric = 'Accuracy'\n","        scores[col] = (score, metric)\n","    average_score = np.mean([score for score, metric in scores.values()])\n","    average_score = round(average_score * 100, 2)\n","    del data\n","    del train_data\n","    del test_data\n","    del prediction_data\n","    del scores\n","    return average_score\n","for csv_file in all_csv_files_sorted:\n","    file_path = os.path.join(root_folder, csv_file)\n","    average_score = process_dataset(file_path)\n","    print(file_path)\n","    print(average_score, '%')\n","    print()\n","    del file_path\n","    del average_score\n","    gc.collect()"]}]}